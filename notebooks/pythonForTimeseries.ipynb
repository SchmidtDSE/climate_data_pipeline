{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8b397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory set to: dataForRScrtips\n",
      "Reprojecting boundary to EPSG:4326 initially...\n",
      "Successfully loaded boundary.\n",
      "\n",
      "--- Processing temp ---\n",
      "Retrieving monthly temp data...\n",
      "Input scenario='SSP 1-2.6' is not a valid option.\n",
      "Closest options: \n",
      "- SSP 5-8.5\n",
      "- SSP 3-7.0\n",
      "- SSP 2-4.5\n",
      "Outputting data for scenario='SSP 5-8.5'\n",
      "\n",
      "  Converted temperature to Celsius.\n",
      "  Reprojecting boundary vector to match raster CRS (PROJCS[\"undefined\",GEOGCS[\"undefined\",DATUM[\"undefined\",SPHEROID[\"undefined\",6370000,0]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"standard_parallel_1\",30],PARAMETER[\"standard_parallel_2\",60],PARAMETER[\"latitude_of_origin\",38],PARAMETER[\"central_meridian\",-70],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]])...\n",
      "  Masking data to boundary (in native CRS)...\n",
      "  Calculating spatial average within the boundary...\n",
      "  Loading data into memory (this might take time)...\n",
      "Calculating annual time series...\n",
      "Calculating anomalies relative to baseline (1995-2014)...\n",
      "Applying 10-year centered rolling average...\n",
      "\n",
      "--- Processing precip ---\n",
      "Retrieving monthly precip data...\n",
      "Input scenario='SSP 1-2.6' is not a valid option.\n",
      "Closest options: \n",
      "- SSP 5-8.5\n",
      "- SSP 3-7.0\n",
      "- SSP 2-4.5\n",
      "Outputting data for scenario='SSP 5-8.5'\n",
      "\n",
      "  Reprojecting boundary vector to match raster CRS (PROJCS[\"undefined\",GEOGCS[\"undefined\",DATUM[\"undefined\",SPHEROID[\"undefined\",6370000,0]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"standard_parallel_1\",30],PARAMETER[\"standard_parallel_2\",60],PARAMETER[\"latitude_of_origin\",38],PARAMETER[\"central_meridian\",-70],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]])...\n",
      "  Masking data to boundary (in native CRS)...\n",
      "  Calculating spatial average within the boundary...\n",
      "  Loading data into memory (this might take time)...\n",
      "Calculating annual time series...\n",
      "Calculating anomalies relative to baseline (1995-2014)...\n",
      "  Diagnostic: Precipitation Baseline Mean (Avg across simulations): 244.1818 mm/year\n",
      "Applying 10-year centered rolling average...\n",
      "\n",
      "--- Exporting data to CSV ---\n",
      "  temp data includes 8 simulations.\n",
      "  precip data includes 8 simulations.\n",
      "\n",
      "Successfully saved combined data to dataForRScrtips/JoshuaTree_decadal_anomalies_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import climakitae as ck\n",
    "from climakitae.core.data_interface import get_data\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "#no warninings outputs for now\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "STANDARD_CRS = \"EPSG:4326\" #WGS84\n",
    "#classic vars\n",
    "ANALVARS = {\"temp\": \"Air Temperature at 2m\",\"precip\": \"Precipitation (total)\"}\n",
    "#Just variety of scenarios\n",
    "SCENARIOS = [\"Historical Climate\",\"SSP 3-7.0\",\"SSP 5-8.5\", \"SSP 2-4.5\", \"SSP 1-2.6\"]\n",
    "RES = \"3 km\"\n",
    "TIMESPAN = (1950, 2099)\n",
    "BASELINEPER = (1995, 2014)\n",
    "HISENDYEAR =2014 \n",
    "SMOOTHINGWindow =10\n",
    "ourputDataForR = \"dataForRScrtips\"\n",
    "os.makedirs(ourputDataForR, exist_ok=True)\n",
    "print(f\"Output directory @:{ourputDataForR}\")\n",
    "#grav Jtree Boundary & Ensure it starts in WGS84\n",
    "shapefilePath =\"../JoshuaTreeOutlines/JoshuaTree/Joshua_Tree_National_Park.shp\"\n",
    "try:\n",
    "    jTreeBoundary_WGS84 = gpd.read_file(shapefilePath)\n",
    "    \n",
    "    #GDP has a CRS and it's  WGS84 initially\n",
    "    if jTreeBoundary_WGS84.crs is None:\n",
    "        print(\"Jtree bounding shape file doesn't have CRS-- Assuming WGS84.\")\n",
    "        jTreeBoundary_WGS84.set_crs(STANDARD_CRS, inplace=True)\n",
    "    if str(jTreeBoundary_WGS84.crs) != STANDARD_CRS:\n",
    "        print(f\"Reprojecting boundary to {STANDARD_CRS}\")\n",
    "        jTreeBoundary_WGS84 = jTreeBoundary_WGS84.to_crs(STANDARD_CRS)\n",
    "    bounds =jTreeBoundary_WGS84.total_bounds\n",
    "    longitudinalSlice =(bounds[0], bounds[2])\n",
    "    latitidualSlice= (bounds[1], bounds[3])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"couldn't load Jtree bounding file w/ err:{e}\")\n",
    "    print(\"Making trivial Jtree rectangle instead\")\n",
    "    jTreeBoundary_WGS84 =None\n",
    "    longitudinalSlice =(-116.6,-115.4)\n",
    "    latitidualSlice = (33.5, 34.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496e22c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ANALVARS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 222\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# --- 3. Execution ---\u001b[39;00m\n\u001b[32m    218\u001b[39m \n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Process both variables\u001b[39;00m\n\u001b[32m    220\u001b[39m time_series_data = {}\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, varName \u001b[38;5;129;01min\u001b[39;00m \u001b[43mANALVARS\u001b[49m.items():\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# Pass the WGS84 boundary GDF to the processing function\u001b[39;00m\n\u001b[32m    224\u001b[39m     ts_data = process_variable(\n\u001b[32m    225\u001b[39m         key, varName, SCENARIOS, RES, TIMESPAN, latitidualSlice, longitudinalSlice, jTreeBoundary_WGS84\n\u001b[32m    226\u001b[39m     )\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ts_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'ANALVARS' is not defined"
     ]
    }
   ],
   "source": [
    "# Helpers\n",
    "\n",
    "def convertPrecipUnits(dataArray):\n",
    "    \"\"\"\n",
    "    takes precipitation from 'flux' (kg/m^2/s) to cumulative depth (mm) per month.\n",
    "    \"\"\"\n",
    "    #kg/m^2/s = 1 mm/s\n",
    "    if dataArray.attrs.get('units') in ['kg/m^2/s', 'kg m-2 s-1']:        \n",
    "        #dats in month per time stamp\n",
    "        days_in_month = dataArray.time.dt.days_in_month\n",
    "        seconds_per_day = 86400\n",
    "        #notes for lab meeting on converstion -> (mm/s) *(seconds/day) * (days/month) = mm/month\n",
    "        dataArray = (dataArray *seconds_per_day) *days_in_month\n",
    "        dataArray.attrs['units'] ='mm/month'\n",
    "        return dataArray\n",
    "    return dataArray\n",
    "\n",
    "def maskToBoundary(dataArray,boundaryGDF_WGS84):\n",
    "    \"\"\"\n",
    "    Masks dataarray by reprojecting the Vector boundary to match the RASTER data's CRS, we talked about this in meeting but also \n",
    "    seems to fix erros in high-dimen reprojection\n",
    "    \"\"\"\n",
    "    if dataArray is None or boundaryGDF_WGS84 is None:\n",
    "        return dataArray\n",
    "    dataCrs = dataArray.rio.crs\n",
    "    if dataCrs is None:\n",
    "        print(\"raster has no CRS can't mask\")\n",
    "        return dataArray\n",
    "    #Defensively reproject boundary (represented by vector) to match CRS of the raster.\n",
    "    try:\n",
    "        if str(dataCrs) != str(boundaryGDF_WGS84.crs):\n",
    "            print(f\"reprojecting boundary vector to match raster CRS ({dataCrs})\")\n",
    "            boundaryNativeCRS =boundaryGDF_WGS84.to_crs(dataCrs)\n",
    "        else:\n",
    "            boundaryNativeCRS =boundaryGDF_WGS84\n",
    "    except Exception as e:\n",
    "        print(f\"Error with reprojecting: {e}.\")\n",
    "        return dataArray\n",
    "\n",
    "    #Prep data strucutre: cleanup, then transpose for rioxarray compatibilit\n",
    "    try:\n",
    "        #need to drop conflicting auxiliary coords which have diff dimens\n",
    "        coordsToDrop = ['lat', 'lon','landmask','lakemask']\n",
    "        dataCleaned = dataArray.drop_vars(coordsToDrop, errors='ignore')\n",
    "        #y, x must be last by rioxarray library invariants\n",
    "        try:\n",
    "            #find spatial dimens\n",
    "            spatialDims =(dataCleaned.rio.y_dim, dataCleaned.rio.x_dim)\n",
    "        except Exception:\n",
    "             #fallback iff rioxarray attributes are missing\n",
    "             if 'y' in dataCleaned.dims and 'x' in dataCleaned.dims:\n",
    "                 spatialDims = ('y','x')\n",
    "             else:\n",
    "                print(\"Error: Could not identify spatial dimensions (x/y).\")\n",
    "                print(\"couldn't get spatial dimensions -> x & y\")\n",
    "                return dataArray\n",
    "        nonSpatialDims =[dim for dim in dataCleaned.dims if dim not in spatialDims] #one line filer out all non spatial\n",
    "        expectedOrder = tuple(nonSpatialDims) +spatialDims\n",
    "        if dataCleaned.dims != expectedOrder:\n",
    "            print(\"  Transposing dimensions\")\n",
    "            dataCleaned =dataCleaned.transpose(*expectedOrder)\n",
    "    except Exception as e:\n",
    "        print(f\"  Something went wrong in cleaning and transposing: {e}\")\n",
    "        dataCleaned = dataArray\n",
    "\n",
    "    #Masking == Clipping\n",
    "    try:\n",
    "        maskedData= dataCleaned.rio.clip(\n",
    "            boundaryNativeCRS.geometry.values, \n",
    "            drop=False, #Keep dimens, just mask values outside the boundary\n",
    "            all_touched=True #include partially overlapping pixels\n",
    "        )\n",
    "        maskedData.attrs.update(dataArray.attrs)\n",
    "        return maskedData\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"err w/ masking: {e}\")\n",
    "        return dataCleaned\n",
    "\n",
    "def processVariable(variableKey, variableName, scenarios, resolution, timeSpan, latSlice, longitSice, Boundary_gdf_wgs84):    \n",
    "    #fetch montly data w/ caladapt API\n",
    "    print(f\"getting monthly {variableKey} data\")\n",
    "    try:\n",
    "        data = get_data(variable=variableName, resolution=resolution, #just use the consts defined in cell one\n",
    "            timescale=\"monthly\",scenario=scenarios,\n",
    "            time_slice=timeSpan,latitude=latSlice,longitude=longitSice)\n",
    "        if data is None or data.time.size == 0:\n",
    "            print(\"  no data got returned\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"error in getting {variableKey}: {e}\")\n",
    "        return None\n",
    "\n",
    "    #Preprocess: Convert Units\n",
    "    if variableKey =='temp' and data.attrs.get('units') =='K':\n",
    "        data =data -273.15\n",
    "        data.attrs['units'] ='°C'        \n",
    "    if variableKey =='precip':\n",
    "        data = convertPrecipUnits(data)\n",
    "    #Spatial Processing (Masking & Averaging)\n",
    "    maksedData = maskToBoundary(data, Boundary_gdf_wgs84)\n",
    "    try:\n",
    "        xDim =maksedData.rio.x_dim\n",
    "        yDim = maksedData.rio.y_dim\n",
    "        #calculate spatial avg, skipna=True is neeeded to ignore the masked out areas!\n",
    "        print(\"  getting spatial avg within the boundary\")\n",
    "        spatialAVG =maksedData.mean(dim=[xDim, yDim], skipna=True)\n",
    "    except Exception as e:\n",
    "        print(f\"  error in calculating spatial average: {e}. Just using basic box right now...\")\n",
    "        try:\n",
    "            spatialAVG = data.mean(dim=['x','y'], skipna=True)\n",
    "        except:\n",
    "             print(\"native box also didn't work...gulp\")\n",
    "             return None\n",
    "    #need to load data into memory for Dask computation argh !\n",
    "    print(\"  loading data into memory-- this takes a minutue\")\n",
    "    spatialAVG.load() \n",
    "    \n",
    "    #diagnostic \n",
    "    if spatialAVG.isnull().all():\n",
    "        print(\"  spatical average is all null\")\n",
    "        return None\n",
    "    #temporal aggregation annual means/totals\n",
    "    print(\"Calculating annual time series...\")\n",
    "    #need to use 'YE' -- year end for resampling frequency\n",
    "    if variableKey =='precip':\n",
    "        #annual total precip (sum of monthly accumulations)\n",
    "        annualData = spatialAVG.resample(time='YE').sum(dim='time')\n",
    "        annualData.attrs['units'] ='mm/year'\n",
    "    else:\n",
    "        #annual mean temperature (mean of month)\n",
    "        annualData =spatialAVG.resample(time='YE').mean(dim='time')\n",
    "    #anomoly calculations\n",
    "    print(f\"getting anomalies relative to baseline\")\n",
    "    #Time based baseline selection\n",
    "        #we need to get baseline period pruely based on time\n",
    "        #this handles merged dataset where entire TS might only scare one scenario label\n",
    "    baselineSlice= annualData.sel(time=slice(str(BASELINEPER[0]), str(BASELINEPER[1])))\n",
    "    if baselineSlice.time.size ==0:\n",
    "        print(\"  No data found for the baseline period.\")\n",
    "        return None\n",
    "    #get mean across the beaseline years for each simulation individually.\n",
    "    baseline_mean = baselineSlice.mean(dim='time')\n",
    "    #check baszeline values\n",
    "    if variableKey =='precip':\n",
    "        try:\n",
    "            avg_baseline = baseline_mean.mean().values\n",
    "            print(f\"  precip baseline mean--avg across simulations--: {avg_baseline:.4f} mm/year\")\n",
    "            if np.isnan(avg_baseline) or avg_baseline < 0.1:\n",
    "                 print(\"  err: base precipitation is NaN or super super low (< 0.1mm/year).\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    #get anomalies-- what's nice though is X array auto aligns dimens like simulation\n",
    "    if variableKey == 'precip':\n",
    "        #percent chnage for preip\n",
    "        epsilon = 1e-6 #thresh for near 0 preip\n",
    "        \n",
    "        #percent change directly\n",
    "        anomalies =((annualData -baseline_mean) / baseline_mean) *100\n",
    "        #Handle cases where the baseline is near zero -- like arid region handling\n",
    "            #define change as 0% only iff baseline and future are w/in thresh of 0, and iff basleine is not null\n",
    "            #if baseline is near zero and future is not, anomoly will be large or near inf\n",
    "        isZeroChange = (abs(baseline_mean) <epsilon) &(abs(annualData) < epsilon) &(baseline_mean.notnull())\n",
    "        anomalies =xr.where(\n",
    "            isZeroChange, 0, anomalies )\n",
    "        unit ='% Change' #don't want to keep copy and pasting \n",
    "    else:\n",
    "        #abs for temperature\n",
    "        anomalies = (annualData -baseline_mean)\n",
    "        origUnits =annualData.attrs.get('units','°C')\n",
    "        unit = f\"Δ{origUnits}\" #delta here for clipboard\n",
    "    #genreally as a note mayeb make this more funciton based for different things so i'm not so reliant on prepip \n",
    "    #and temp @ 2 meteres TODO ^^\n",
    "    anomalies.attrs['units'] =unit\n",
    "    # 6. Smoothing (Decadal Mean)\n",
    "    #smoothing via decadal mean\n",
    "    print(f\"appling smoothing window on smoothing dinwo (defined in cell one) window average\")\n",
    "    # w/ min_periods=1 to handle maybe missing years well\n",
    "    smoothed_anomalies =anomalies.rolling(time=SMOOTHINGWindow,center=True, min_periods=1).mean()\n",
    "    return smoothed_anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1acf4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Executing and filling csvs \n",
    "timeSeriesData ={}\n",
    "for key, varName in ANALVARS.items():\n",
    "    #pass the WGS84 boundary GDF to the processing function\n",
    "    tsData =processVariable(\n",
    "        key, varName, SCENARIOS, RES, TIMESPAN, latitidualSlice, longitudinalSlice, jTreeBoundary_WGS84)\n",
    "    if tsData is not None:\n",
    "        timeSeriesData[key] =tsData\n",
    "#exporting to csv\n",
    "if not timeSeriesData:\n",
    "    print(\"\\no data was processed so exiting.\")\n",
    "else:\n",
    "    print(\"\\ngetting data into csv.\")\n",
    "    allDFs = []\n",
    "    for varKey, dataArrat in timeSeriesData.items():\n",
    "        #converting x array data to pd df\n",
    "            #there's a helpful func which apparently does this p wel\n",
    "        df = dataArrat.to_dataframe(name='Anomaly').reset_index()\n",
    "        #add var name to the column\n",
    "        df['Variable'] = varKey\n",
    "        #clean up fs for R tidy format\n",
    "        if 'time'in df.columns:\n",
    "            df['Year'] = df['time'].dt.year\n",
    "            df = df.drop(columns=['time'])\n",
    "        #post processing now-- standardize scenario names\n",
    "        #handle merged names like : \"Historical + SSP 3-7.0\" by relabling based on year\n",
    "        if 'scenario' in df.columns:\n",
    "            #clean up raw scenario names, remove historical + prefix iff present\n",
    "            #ensure they're strings\n",
    "            stringToRmove = \"Historical +\"\n",
    "            sspName =df['scenario'].astype(str).str.replace(stringToRmove, \"\", regex=False)\n",
    "            #det final scenario labe based on year\n",
    "            df['Scenario'] =np.where(\n",
    "                df['Year']<= HISENDYEAR,\n",
    "                \"Historical Climate\", sspName)\n",
    "            #ensure that standalone historical climate runs iff retrieved separetely are preserved\n",
    "            #handles cases where ssp_name derived above might already be \"Historical Climate\" for exampel\n",
    "            df.loc[df['scenario'] == \"Historical Climate\", 'Scenario'] =\"Historical Climate\"\n",
    "            #clean up orig scenario column\n",
    "            df = df.drop(columns=['scenario'], errors='ignore')\n",
    "        if 'simulation' in df.columns:\n",
    "            df = df.rename(columns={'simulation': 'Simulation'})\n",
    "        # Two rukles\n",
    "            #drop Nan/inf vals resulting from rolling windows or anomaly calcs\n",
    "            #replace Inf/-Inf with NaN first, then drop all NaNs.\n",
    "        dfCleaned = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['Anomaly'])\n",
    "        if dfCleaned.empty and not df.empty:\n",
    "            print(f\"  uh oh:all anomaly vals for {varKey} were nan or inf and have been dropped.\")\n",
    "        allDFs.append(dfCleaned)\n",
    "        #print sims included\n",
    "        if 'Simulation' in dfCleaned.columns:\n",
    "            print(f\"  {varKey} data has {len(dfCleaned['Simulation'].unique())} sims.\")\n",
    "    if allDFs:\n",
    "        combinedDf = pd.concat(allDFs)\n",
    "        if not combinedDf.empty:\n",
    "            #reorder cols\n",
    "            try:\n",
    "                cols =['Year', 'Scenario', 'Simulation', 'Variable', 'Anomaly']\n",
    "                combinedDf = combinedDf[cols]\n",
    "            except KeyError:\n",
    "                print(\"reordering failed, some columns that were expected weren't there\")\n",
    "            filename = os.path.join(ourputDataForR, \"JoshuaTree_decadal_anomalies_combined.csv\")\n",
    "            combinedDf.to_csv(filename, index=False)\n",
    "            print(f\"\\CSV FOR R is called!: {filename}\")\n",
    "        else:\n",
    "            print(\"\\combiled df empty emptt, nothign saved\")\n",
    "    else:\n",
    "        print(\"\\n no data was generated to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
